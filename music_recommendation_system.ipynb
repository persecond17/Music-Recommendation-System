{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fZ7Lks-0bIM",
        "outputId": "f4af5ff5-84af-46ff-9eb2-699096574040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path='drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mFz065WfvB4z"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark\n",
        "# !pip install pymongo\n",
        "# !pip install spotipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0Eno-Xcad8"
      },
      "source": [
        "Drawing inspiration from social media music sharing, we've observed that modern individuals frequently use music as a medium for personal expression, enthusiastically sharing and showcasing their preferences. Intriguingly, this implies a reverse correlation as well - the sentiments and language used in song-sharing social media posts can provide significant insights into the character of the songs themselves. \n",
        "\n",
        "Motivated by this perspective, we are eager to delve deeper into this phenomenon and integrate it into our music recommendation engine. Our goal is to develop a content-based recommendation system, trained on social media content (particularly tweets), capable of generating tailored playlists from any input text. This approach will infuse our music recommendation system with an element of novelty and unexpected delight.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM9WU_tSclpf"
      },
      "source": [
        "# [Table of contents]()\n",
        "### [0. Pre-processing](#pre)\n",
        "##### [Import packages](#import)\n",
        "##### [Connect with MongoDB and initialize a SparkSession](#connect)\n",
        "##### [Toolkits: Pre-defined functions](#toolkits)\n",
        "\n",
        "### [1. Data Preparation](#data)\n",
        "##### [Initial the journey ðŸ¥³](#initial)\n",
        "##### [Data Cleaning](#clean)\n",
        "##### [Data Spliting](#split)\n",
        "##### [Data Recombination](#recomb)\n",
        "\n",
        "### [2. Model Initializing: TF-IDF](#tfidf)\n",
        "##### [Setup: Utilizing hashed TF-IDF](#setup)\n",
        "##### [Evaluation: Top 15 accuracy achieved 88%](#evaluation)\n",
        "##### [Prediction: Recommend songs based on text input](#prediction)\n",
        "\n",
        "### [3. Gaming time ðŸ˜Ž](#merge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lniOVdGQe0N3"
      },
      "source": [
        "# Pre-processing <a name=\"pre\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h650-GHIfhz-"
      },
      "source": [
        "## Import packages <a name=\"import\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH4IJqI6-NoZ",
        "outputId": "5970eb4e-6058-44bc-d919-9755186fc775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Python standard libraries\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import itertools\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# External general libraries\n",
        "import numpy as np\n",
        "import requests\n",
        "import pymongo\n",
        "import pyspark\n",
        "from bs4 import BeautifulSoup\n",
        "from scipy.stats import ttest_1samp, ttest_ind\n",
        "from google.cloud import storage\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Spark libraries\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import StringIndexer, MinMaxScaler, MaxAbsScaler, Normalizer, HashingTF, IDF, IDFModel, Tokenizer\n",
        "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
        "\n",
        "# Music-related libraries\n",
        "import spotipy\n",
        "import spotipy.util as util\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "\n",
        "# Suppress warnings \n",
        "warnings.filterwarnings(action = 'ignore') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXpJf2KBQvvn"
      },
      "source": [
        "## Connect with MongoDB and initialize a SparkSession <a name=\"connect\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQA80F_x0Sq"
      },
      "source": [
        "### Connect to MongoDB Atlas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mongo_username = 'your_mongo_username'\n",
        "mongo_password = 'your_mongo_password'\n",
        "mongo_ip_address = 'your_database_name.lasvt.mongodb.net/?retryWrites=true&w=majority'\n",
        "database_name = 'your_database_name'\n",
        "collection_name = 'your_collection_name'\n",
        "client = pymongo.MongoClient(f\"mongodb+srv://{mongo_username}:{mongo_password}@{mongo_ip_address}\")\n",
        "db = client[database_name]\n",
        "collection = db[collection_name]"
      ],
      "metadata": {
        "id": "hEmSF0WeA1it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o82846dmKYLX"
      },
      "source": [
        "### Create a SparkSession object and a SparkContext object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNf75sjcuQqZ"
      },
      "outputs": [],
      "source": [
        "blob_name='whole.csv'\n",
        "bucket_name='spotify-twitter'\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.network.timeout\", \"360000000s\")\\\n",
        "                            .config(\"spark.executor.heartbeatInterval\", \"3600s\")\\\n",
        "                            .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGsy-_EpRCKD"
      },
      "source": [
        "## Toolkits: Pre-defined functions <a name=\"toolkits\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHJmHW66huvr"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove any emojis.\n",
        "    \"\"\"\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', str(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WzwTU55hv1X"
      },
      "outputs": [],
      "source": [
        "# Create instances of stemmers and lemmatizers\n",
        "ps = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "def stemming(word: str) -> str:\n",
        "    \"\"\"\n",
        "    Lemmatize the input word to its base or dictionary form.\n",
        "    Stem the lemmatized word to its root form.\n",
        "    \"\"\"\n",
        "    lemmed_word = wordnet_lemmatizer.lemmatize(word)\n",
        "    stemmed_word = snowball_stemmer.stem(lemmed_word)\n",
        "    return stemmed_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reasP5fCh67G"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Normalize each word to lowercase, strip punctuation,\n",
        "    remove stop words, drop words of length <= 2, strip digits.\n",
        "    Stem text and return a list of tokenized words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', str(text))\n",
        "    tokens = text.split(' ')\n",
        "    tokens = [w for w in tokens if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    # tokens = [stemming(token) for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXnAOPH1gLgE"
      },
      "outputs": [],
      "source": [
        "def simple_tokenize(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize each word to lowercase, strip punctuation, \n",
        "    return a list of words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', str(text))\n",
        "    tokens = text.split(' ')\n",
        "    tokens = [w for w in tokens if len(w) > 0]  # ignore a, an, to, at, be, ...\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJDkalOnWid"
      },
      "outputs": [],
      "source": [
        "def converting_input(input_text: str):\n",
        "    \"\"\"\n",
        "    Split input_text into words.\n",
        "    Duplicate input until it has at least 15 \n",
        "    words for better model identification.\n",
        "    \"\"\"\n",
        "    input = tokenize(input_text)\n",
        "    while len(input_text) < 15:\n",
        "        input *= 5\n",
        "    df_input = spark.createDataFrame([[input]],[\"raw\"])\n",
        "    return df_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g43as4TkRLO5"
      },
      "outputs": [],
      "source": [
        "cols_list = [\"popularity\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \n",
        "             \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"]\n",
        "\n",
        "def get_songs_docs(collection):\n",
        "    \"\"\"\n",
        "    Collect corpus from all tweets csv files stored in GCS.\n",
        "    Filter tweets published by ads account (with 'http', '@').\n",
        "    Parse music features and corresponding tweets for each song.\n",
        "    Delete songs without tweets.\n",
        "    \"\"\"\n",
        "    corpus, corpus2, corpus3 = [], [], []\n",
        "    name_list, id_list = [], []\n",
        "    drop_name, drop_id = [], []\n",
        "    drop_num = 0\n",
        "    total_tweets = collection.aggregate([{\"$match\": {\"tweets\": {\"$exists\": True}}},\n",
        "                                         {\"$group\": {\"_id\": 0, \"count\": {\"$sum\": {\"$size\": \"$tweets\"}}}}])\n",
        "    origin_dict = list(collection.find({'tweets':{'$ne':[]}},\n",
        "                                       {'_id':1,'tweets':1,'name':1,'artist':1}))\n",
        "    \n",
        "    # Count songs without tweets\n",
        "    for line in origin_dict:\n",
        "        try: \n",
        "            tweets = line['tweets']\n",
        "        except: \n",
        "            drop_num += 1\n",
        "            drop_name.append(line['name'])\n",
        "            drop_id.append(line['_id'])\n",
        "            continue\n",
        "\n",
        "        # Parse song name, artist, tweets and other features\n",
        "        name = line['name']\n",
        "        id = line['_id']\n",
        "        artist = line['artist']\n",
        "        filter_words = tokenize(name) + tokenize(artist)\n",
        "        name_list.append(name)\n",
        "        id_list.append(id)\n",
        "        docs = sc.parallelize(tweets)\\\n",
        "                    .map(lambda x:x['content'])\\\n",
        "                    .filter(lambda x: isinstance(x, str))\\\n",
        "                    .map(remove_emojis)\\\n",
        "                    .map(lambda x: re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', str(x)))\n",
        "        for fword in filter_words:\n",
        "            docs = docs.map(lambda x: x.replace(fword, ''))\n",
        "\n",
        "        docs_lines = docs.collect()\n",
        "        corpus3 += zip([id]*len(docs_lines), docs_lines)\n",
        "\n",
        "    del origin_dict\n",
        "    \n",
        "    # Assemble dataframe with song name and tokenized tweets\n",
        "    lineData = spark.createDataFrame(corpus3, [\"id\", \"tweet_line\"])\n",
        "    df_fea = spark.createDataFrame(collection.find({'tweets':{'$ne':[]}}))\\\n",
        "              .drop('tweets').cache()\n",
        "    for col in cols_list:\n",
        "        df_fea = df_fea.withColumn(col, df_fea[col].cast(\"float\"))\n",
        "\n",
        "    return lineData, df_fea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y35dPRoki-dw"
      },
      "outputs": [],
      "source": [
        "def scrape_song_lyrics(df_features):\n",
        "    \"\"\"\n",
        "    Scrape lyrics from spotify websites with given dataframe which contains\n",
        "    the ids, names and other metrics of the songs.\n",
        "    Return a dictionary stores ids as keys and lyric lists as values.\n",
        "    Each row of the lyrics is an individual element of the list.\n",
        "    \"\"\"\n",
        "    # Collect the ids of required songs\n",
        "    list_trackid = df_features.select('_id').rdd.map(lambda x:list(x)[0]).collect()\n",
        "    d={}\n",
        "    lines=[]\n",
        "    words=[]\n",
        "   \n",
        "    # Prepare the Spotify auth credentials to access the websites\n",
        "    username='Your username'\n",
        "    pw=\"Your password\"\n",
        "    auth = (username, pw)\n",
        "    for i,trackid in enumerate(list_trackid):\n",
        "\n",
        "      # To avoid being blocked by spotify\n",
        "      if i%5 == 0:\n",
        "        time.sleep(4) \n",
        "\n",
        "      html = 'https://open.spotify.com/track/' + trackid\n",
        "      r = requests.get(html, auth = auth)\n",
        "      soup = BeautifulSoup(r.content,features=\"html.parser\")\n",
        "\n",
        "      # Locate the lyrics in the html files\n",
        "      if len(soup.find_all('h2')) > 0:\n",
        "        lyric_tag=soup.find_all('h2')[0]\n",
        "        if lyric_tag.string == 'Lyrics':\n",
        "            for line in lyric_tag.find_parent(\"div\").findAll(\"p\"):\n",
        "                if isinstance(line.string, str) and line.string.count(' ') > 2:\n",
        "                    lines.append({'id':str(trackid), 'lyric_line': str(line.string)})\n",
        "    line_lyric=spark.createDataFrame(lines, [\"id\", \"lyric_line\"])\n",
        "\n",
        "    return line_lyric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6tSN-a9kDZD"
      },
      "source": [
        "# Data Preparation <a name=\"data\"></a>\n",
        "\n",
        "To ensure that we obtain songs with sufficient tweet content and are widely discussed by people, we have selected playlists that are composed of popular songs or have a tendency to go viral. These playlists include both officially created playlists and playlists that contain top songs from different genres such as jazz, pop, and others. Additionally, we have filtered the songs by popularity, a metric obtained from the Spotify API, by keeping only those **songs with a popularity score of 60 or higher**. This ensures that we are working with songs that have been well-received and have a considerable amount of engagement among listeners.\n",
        "\n",
        "Our project leverages the power of the `Spotify API` and `Twitter API` to collect and analyze data on popular songs and associated tweets. We've implemented a daily data auto-collection pipeline on Airflow, which allows us to efficiently collect and store large volumes of data in **Google Cloud Storage (GCP)** and manage them with **MongoDB Atlas**, a NoSQL database that offers scalable and flexible data storage. \n",
        "\n",
        "Over a span of three months, from February to April, we successfully gathered a substantial dataset consisting of **2k songs, 240k tweets, and 84k lyric lines**. To bolster the precision of our recommendation engine, a meticulous filtration process was employed. We pruned songs with less than 50 related tweets, potential marketing tweets, and their corresponding lyrics. As a result, our refined dataset comprised of 63k tweets and 54k lyric lines. Through meticulous data curation and the establishment of a robust data collection framework, we've ensured our music recommendation engine has the most pertinent and insightful input at its disposal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqiI1bDEdFXP"
      },
      "source": [
        "## Initial the journey ðŸ¥³  <a name=\"initial\"></a>\n",
        "\n",
        "We start by retrieving all the stored data from MongoDB and then scrape the lyrics for each song using the Spotify API. Next, we merge the data into PySpark DataFrames, where we record the song ID and content in each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6nJFXpc3MLl",
        "outputId": "153d3a51-b948-46bd-dc6c-141ce5b45b87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1838, 236973)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fetch tweets and features from mongodb\n",
        "whole_tweet_df_ori, df_features = get_songs_docs(collection)\n",
        "df_features.count(), whole_tweet_df_ori.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "toBfbR6xbb4i",
        "outputId": "73bcd8b9-04e5-4efc-dc63-044909d2326c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83765"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# scrape lyrics from spotify API\n",
        "whole_lyric_df_ori = scrape_song_lyrics(df_features)\n",
        "whole_lyric_df_ori.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Nb2bXd3uq9"
      },
      "outputs": [],
      "source": [
        "# save the dataframes\n",
        "whole_tweet_df_ori.write.save(file_path+'whole_tweet_df_ori')\n",
        "whole_lyric_df_ori.write.save(file_path+'whole_lyric_df_ori')\n",
        "df_features.write.save(file_path+'df_features')\n",
        "\n",
        "# load the dataframes\n",
        "# whole_tweet_df_ori = spark.read.load(file_path+'whole_tweet_df_ori')\n",
        "# whole_lyric_df_ori = spark.read.load(file_path+'whole_lyric_df_ori')\n",
        "# df_features = spark.read.load(file_path+'df_features')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AobYU-FJCvDg"
      },
      "source": [
        "## Data Cleaning <a name=\"clean\"></a>\n",
        "\n",
        "In this step, we will identify and drop tweets from robots, tweets with insufficient information, duplicated tweets by PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJpReFZDCutB",
        "outputId": "12c204e0-1c41-478c-ab93-56b9917784d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of whole_tweet_df from 236973 to 63367, narrowed down 73.3%.\n"
          ]
        }
      ],
      "source": [
        "# tweets cleaning: drop duplicate tweets; tweets containing stopwords; delete the songs that don't have enough tweets\n",
        "\n",
        "s1 = whole_tweet_df_ori.count()\n",
        "whole_tweet_df = whole_tweet_df_ori.dropDuplicates()\n",
        "udf_wordcount=udf(lambda x: int(len(simple_tokenize(x))), IntegerType())\n",
        "whole_tweet_df = whole_tweet_df.withColumn('word_count',udf_wordcount(whole_tweet_df.tweet_line))\n",
        "whole_tweet_df = whole_tweet_df.filter(whole_tweet_df.word_count>10)\n",
        "whole_tweet_df = whole_tweet_df.filter((~whole_tweet_df.tweet_line.contains('playing'))\\\n",
        "                                       &(~whole_tweet_df.tweet_line.contains('amp'))\\\n",
        "                                       &(~whole_tweet_df.tweet_line.contains('hps:'))\\\n",
        "                                       &(~whole_tweet_df.tweet_line.contains('http'))\\\n",
        "                                       &(~whole_tweet_df.tweet_line.contains('@')))\n",
        "filter_songs_id = whole_tweet_df.groupby('id').count().filter(col('count')>10).select('id').collect()\n",
        "filter_songs_id_list = [r.asDict()['id'] for r in filter_songs_id]\n",
        "whole_tweet_df = whole_tweet_df.filter(col('id').isin(filter_songs_id_list))\n",
        "s2 = whole_tweet_df.count()\n",
        "print(f\"The length of whole_tweet_df from {s1} to {s2}, narrowed down {(s1-s2)/s1*100:.1f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A9bj3QBB9sE",
        "outputId": "77d365aa-331a-4a2c-879e-e3caf5a32760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of whole_lyrics_df from 83765 to 54455, narrowed down 35.0%.\n"
          ]
        }
      ],
      "source": [
        "# lyric cleaning: drop the lines contains only some duplicate words\n",
        "\n",
        "t1 = whole_lyric_df_ori.count()\n",
        "unique_num_udf = udf(lambda x: len(list(set(simple_tokenize(x)))), IntegerType())\n",
        "whole_lyrics_df = whole_lyric_df_ori.dropDuplicates()\n",
        "whole_lyrics_df = whole_lyrics_df.withColumn('lyric_word_num',unique_num_udf(whole_lyrics_df.lyric_line))\n",
        "whole_lyrics_df = whole_lyrics_df.filter(whole_lyrics_df.lyric_word_num>1).drop('lyric_word_num')\n",
        "t2 = whole_lyrics_df.count()\n",
        "print(f\"The length of whole_lyrics_df from {t1} to {t2}, narrowed down {(t1-t2)/t1*100:.1f}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF__mC6VPFQn"
      },
      "source": [
        "## Data Spliting <a name=\"split\"></a>\n",
        "\n",
        "To ensure that each song's corpus appears in both the training and test sets, we applied a stratified split on the \"id\" column for each dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDbx32kXtzzF",
        "outputId": "7dc45d68-c2f0-431b-87ec-b6157debe84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lyrics: \t The length of train data: 48950 \t The length of test data: 5505\n",
            "Tweets: \t The length of train data: 57008 \t The length of test data: 6359\n"
          ]
        }
      ],
      "source": [
        "# stratified sampling on tweet_lyric_df\n",
        "def stratified_split_train_test(df, frac, label, seed=42):\n",
        "    \"\"\" stratfied split of a dataframe in train and test set.\"\"\"\n",
        "    df=df.withColumn(\"index\", monotonically_increasing_id())\n",
        "    fractions = df.select(label).distinct().withColumn(\"fraction\", lit(frac)).rdd.collectAsMap()\n",
        "    df_frac = df.stat.sampleBy(label, fractions, seed)\n",
        "    df_remaining = df.join(df_frac, on='index', how=\"left_anti\")\n",
        "    return df_frac, df_remaining\n",
        "\n",
        "test_lyric_df, train_lyric_df = stratified_split_train_test(whole_lyrics_df, 0.1, 'id')\n",
        "test_tweet_df, train_tweet_df = stratified_split_train_test(whole_tweet_df, 0.1, 'id')\n",
        "\n",
        "print(f\"Lyrics: \\t The length of train data: {train_lyric_df.count()} \\t The length of test data: {test_lyric_df.count()}\")\n",
        "print(f\"Tweets: \\t The length of train data: {train_tweet_df.count()} \\t The length of test data: {test_tweet_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NZFIqqKOww"
      },
      "source": [
        "## Data Recombination <a name=\"recomb\"></a>\n",
        "\n",
        "In an initial step, we consolidated tweet lines and lyric lines related to specific songs from the training data into two data frames, `train_tweet_df` and `train_lyric_df`. Each line was grouped by song ID and merged into a singular text string. \n",
        "\n",
        "Following this, we performed an 'inner' join on these frames to unify lyrics and tweets for each song. Leveraging a user-defined function, the collective content was then tokenized and stemmed. Subsequently, we once again combined lyrics and tweets, generating a fresh `tweet_lyric_cor` column within a data frame destined for model training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdoLlSkEKOJY",
        "outputId": "7b8dbf6e-fc83-4a63-f0ba-456e802f5129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of merged data: 889\n"
          ]
        }
      ],
      "source": [
        "# concat lines of tweets / lyrics from the same song into a single line separately\n",
        "train_tweet_corpus_df = train_tweet_df.groupby(\"id\").agg(concat_ws(\" \", collect_list(train_tweet_df.tweet_line)).alias('tweet_cor')).withColumnRenamed('id','ID_')\n",
        "train_lyric_corpus_df = train_lyric_df.groupby(\"id\").agg(concat_ws(\" \", collect_list(train_lyric_df.lyric_line)).alias('lyric_cor'))\n",
        "\n",
        "# join, concat lyric and tweets and split into words\n",
        "lyirc_tweet_corpus_df = train_tweet_corpus_df.join(train_lyric_corpus_df, train_lyric_corpus_df.id==train_tweet_corpus_df.ID_, 'inner')\n",
        "udf_tokenize = udf(lambda x: [stemming(i) for i in tokenize(x)], ArrayType(StringType()))\n",
        "lyirc_tweet_corpus_concat_df = lyirc_tweet_corpus_df.select(concat_ws(' ', lyirc_tweet_corpus_df.tweet_cor, lyirc_tweet_corpus_df.lyric_cor).alias('tweet_lyric_cor'),'id')\n",
        "lyirc_tweet_corpus_split_df = lyirc_tweet_corpus_concat_df.select('id', udf_tokenize(lyirc_tweet_corpus_concat_df.tweet_lyric_cor).alias('raw'))\n",
        "\n",
        "print(f\"The length of merged data: {lyirc_tweet_corpus_split_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the dataframes\n",
        "lyirc_tweet_corpus_split_df.write.save(file_path+'lyirc_tweet_corpus_split_df')\n",
        "\n",
        "# load the dataframes\n",
        "# lyirc_tweet_corpus_split_df = spark.read.load(file_path+'lyirc_tweet_corpus_split_df')"
      ],
      "metadata": {
        "id": "egF5r5csMQqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gZ5OSh6Yx1D"
      },
      "source": [
        "# Model Initializing: TF-IDF <a name=\"tfidf\"></a>\n",
        "\n",
        "Harnessing the strength of the TFIDF (Term Frequency-Inverse Document Frequency) methodology, which effectively quantifies the importance of a word in a document within a large corpus, we devised an approach to encapsulate the essence of each song's content.\n",
        "\n",
        "In our approach towards calculating the TFIDF for each song, we initially merged tweets and lyrics with the same song ID from the training dataset. This harmonized data was then combined, forming a comprehensive corpus for every song. Following this, we hashed the tokenized and stemmed content of each song, consequently generating their respective TFIDF vectors. In a bid to ensure that each hashing value is unique to a single word, we elected to employ a substantial number of features, set at 100,000.\n",
        "\n",
        "When predicting based on a text input, we tokenize and stem the input, converting it into a hashed vector. This vector is then compared with each song's TFIDF vector through a dot product operation. The values obtained from this process, indicative of the correlation between the input and each song, are arranged in descending order. We then curate a recommendation playlist comprising the songs possessing the highest correlational values, i.e., the ones most in tune with the input text.\n",
        "\n",
        "We subjected our methodology to rigorous testing on a subset of 50 songs from our test set. For each song, we fed the corresponding tweet corpus into our system and checked whether the target song featured among the top 15 recommended songs. This process resulted in an impressive 'top 15 accuracy' of 88%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PkqTxmkk94o"
      },
      "source": [
        "## Setup: Utilizing hashed TF-IDF <a name=\"setup\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwgBhKORinHf"
      },
      "outputs": [],
      "source": [
        "def tfidf_model(df, numFeatures=100000):\n",
        "    \"\"\"\n",
        "    Train a TF model and a IDF model.\n",
        "    Transform the input using tf_md and idf_md.\n",
        "    \"\"\"\n",
        "    # tf\n",
        "    input_name=df.columns[1]\n",
        "    tf_md = HashingTF(inputCol=input_name, outputCol=\"rawFeatures\", numFeatures=numFeatures)\n",
        "    featurizedData = tf_md.transform(df)\n",
        "\n",
        "    # idf\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    idf_md = idf.fit(featurizedData)\n",
        "    rescaledData = idf_md.transform(featurizedData)\n",
        "    return tf_md, idf_md, rescaledData\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx_qgYsXR7Bq"
      },
      "outputs": [],
      "source": [
        "def target_rank(input_text,target_id):\n",
        "    \"\"\"\n",
        "    Tokenize and stem the sentence then transfrom the input to hashing vector. \n",
        "    Then dot product the hashing vector with each song's tfidf vector, rank the \n",
        "    product value in descending order. Select top ten as the final recommendation.\n",
        "    \n",
        "    Return the rank of target song in the recommendation playlist.\n",
        "    If the target song is not in the playlist, return 100000.\n",
        "    \"\"\"\n",
        "    input_vec = tf_md.transform(converting_input(input_text)).head().rawFeatures\n",
        "    udf_sv_product = udf(lambda x: float(input_vec.dot(x)), FloatType())\n",
        "    rescaledData_pd = rescaledData.select('id',udf_sv_product(rescaledData.features).alias('dot_pd'))\n",
        "    res_id = [x.asDict()['id'] for x in rescaledData_pd.sort(rescaledData_pd.dot_pd.desc()).select('id').head(10)]\n",
        "    if target_id in res_id:\n",
        "        target_index = res_id.index(target_id)\n",
        "        return int(target_index)\n",
        "    else:\n",
        "        return 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwnZuSeI1bz6"
      },
      "outputs": [],
      "source": [
        "def top15_acc(test_tweet_df, target_rank, test_size):\n",
        "    \"\"\"\n",
        "    Evaluate the recommendation performance by calculating the \"top 15 accuracy\", \n",
        "    which is the proportion of instances where the target song is among the top \n",
        "    15 recommended songs based on a given tweet from the test data.\n",
        "    \"\"\"\n",
        "    udf_target_rank = udf(target_rank, IntegerType())\n",
        "    inputList = test_tweet_df.rdd.map(lambda x: x.asDict()['tweet_line']).collect()\n",
        "    targetList = test_tweet_df.rdd.map(lambda x: x.asDict()['id']).collect()\n",
        "    ranklist = []\n",
        "    for i in range(test_size):\n",
        "        if inputList[i] and len(''.join(inputList[i].split(' ')))>1:\n",
        "            x = target_rank(inputList[i], targetList[i])\n",
        "            ranklist.append(x)\n",
        "    acc = np.sum(np.array(ranklist) <= 15) / len(ranklist)\n",
        "    return ranklist, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWfrW0vdhxvC"
      },
      "source": [
        "## Evaluation: Top 15 accuracy achieved 88% <a name=\"evaluation\"></a>\n",
        "\n",
        "To thoroughly gauge the effectiveness of our recommendations, we've adopted \"top 15 accuracy\" as the evaluation metric for our recommendation system. We feed the system with tweet corpus corresponding to each song in the test set. If the top 15 recommended songs include the target song, we deem the recommendation as successful. This metric enables us to quantify the proportion of successful recommendations among all the tested songs. \n",
        "\n",
        "Upon testing with 50 songs, the system demonstrated a robust **\"top 15 accuracy\" of 88.0%**. This score signifies that, for 88.0% of the test instances, the target song featured within the top 15 recommendations, demonstrating the system's promising precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obws9agCFgCZ"
      },
      "outputs": [],
      "source": [
        "# Define the parameter and calculate the tfidf scores for each song. Stored in dataframe.\n",
        "tf_md, idf_md, rescaledData = tfidf_model(lyirc_tweet_corpus_split_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbwfk09b8bJz",
        "outputId": "f74fc363-2e49-4f1b-d0f4-ac27ff5bd59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 15 accuracy for the recommendation system is 88.0%\n"
          ]
        }
      ],
      "source": [
        "# Final Accuracy (super slow)\n",
        "test_size = 50\n",
        "ranklist, accuracy = top15_acc(test_tweet_df, target_rank, test_size)\n",
        "print(f'The top 15 accuracy for the recommendation system is {accuracy*100:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7MVdpy2HwLu"
      },
      "source": [
        "## Prediction: Recommend songs based on text input <a name=\"prediction\"></a>\n",
        "\n",
        "In our prediction process, the input text is first subjected to tokenization and stemming, thus transforming it into a corresponding hashed vector. This vector is then systematically cross-compared, via a dot product operation, with the TFIDF vector of each song in the existing database. \n",
        "\n",
        "The resulting values, representing the degree of correlation between the input and each song, are subsequently ranked in descending order. From this ordered list, we select the songs with the highest correlational values to craft the final recommendation playlist.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_recommendation_3(input_text, tf_md, idf_md, rescaledData, recommend_num, df_features):\n",
        "    \"\"\"\n",
        "    Process an input text and returns a certain number of recommended songs based \n",
        "    on the TF-IDF values, by calculating the dot product between the input vector \n",
        "    and each song's TF-IDF vector, and then selecting the top recommend_num songs \n",
        "    with the highest dot product values.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    input_dataframe= converting_input(input_text)\n",
        "    input_vec = tf_md.transform(input_dataframe).head().rawFeatures\n",
        "    udf_sv_product = udf(lambda x: float(input_vec.dot(x)), FloatType())\n",
        "    rescaledData_pd = rescaledData.select('id', udf_sv_product(rescaledData.features).alias('dot_pd')).toPandas()\n",
        "    res_id=list(rescaledData_pd.nlargest(recommend_num, columns='dot_pd')['id'].to_numpy())\n",
        "\n",
        "    # res_id = [x.asDict()['id'] for x in rescaledData_pd.sort(rescaledData_pd.dot_pd.desc()).select('id').head(recommend_num)]\n",
        "    print(f'This recommendation spent {time.time() - start_time:.1f} seconds.')\n",
        "    return df_features.filter(col('_id').isin(res_id)).select('name')"
      ],
      "metadata": {
        "id": "xFPEESgLbhow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z7utGwL93uj",
        "outputId": "7255a1f7-c34e-44c8-a576-9265266ac830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This recommendation spent 74.9 seconds.\n",
            "+------------------------------------------------+\n",
            "|name                                            |\n",
            "+------------------------------------------------+\n",
            "|I Donâ€™t Wanna Live Forever (Fifty Shades Darker)|\n",
            "|Deep Down (feat. Never Dull)                    |\n",
            "|1-800-273-8255                                  |\n",
            "|You                                             |\n",
            "|just wanna rock (Lil Uzi Vert) - Sped Up Version|\n",
            "|Our House                                       |\n",
            "|Yeah! (feat. Lil Jon & Ludacris)                |\n",
            "|Ride                                            |\n",
            "|Summer                                          |\n",
            "|Cruel Summer                                    |\n",
            "+------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "recommend_num = 10\n",
        "rescaledData = rescaledData.repartition(recommend_num)\n",
        "input_text = 'beautiful summer, wanna make a ride into mountains and find fountains'\n",
        "\n",
        "tfidf_recommendation_3(input_text, tf_md, idf_md, rescaledData, recommend_num, df_features).show(recommend_num, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDfd57uhWrzX"
      },
      "source": [
        "# Gaming time ðŸ˜Ž <a name=\"merge\"></a>\n",
        "\n",
        "Having successfully encapsulated our model and hashed vectors, we're all set to reload them and bring our music recommendation system to life! So, let's dive in and explore the unique playlists our system can curate with different text inputs. Ready for some personalized musical discovery? Let's hit the play button!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XftCuvR_toDP"
      },
      "outputs": [],
      "source": [
        "def recommend_system(input, file_df_path, recommend_num):\n",
        "    \"\"\"\n",
        "    Load stored dataframes and fit the TFIDF model,\n",
        "    then make prediction based on the input sentence.\n",
        "    \"\"\"\n",
        "    # Load_files\n",
        "    df = spark.read.load(file_df_path[0])\n",
        "    df_features = spark.read.load(file_df_path[1])\n",
        "\n",
        "    # Fit tfidf models\n",
        "    tf_md, idf_md, rescaledData = tfidf_model(df)\n",
        "    res = tfidf_recommendation_3(input, tf_md, idf_md, rescaledData, recommend_num, df_features)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_df_path = [file_path+'lyirc_tweet_corpus_split_df', file_path+'df_features']\n",
        "recommend_num = 15"
      ],
      "metadata": {
        "id": "4wcoYZTS0De0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYK6Cjq44nZS"
      },
      "source": [
        "Let's generate playlist for any text input!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNkL53jlmPNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5bc8f4-9eed-4cdf-9ce6-eae4812bc21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This recommendation spent 10.6 seconds.\n",
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|I Wanna Dance wit...|\n",
            "|       Summer Of '69|\n",
            "|                 You|\n",
            "|just wanna rock (...|\n",
            "|Imagine - Remaste...|\n",
            "|           Our House|\n",
            "|Yeah! (feat. Lil ...|\n",
            "|                Ride|\n",
            "|              Summer|\n",
            "|        Cruel Summer|\n",
            "|Players - DJ Smal...|\n",
            "|I Donâ€™t Wanna Liv...|\n",
            "|    I Wanna Be Yours|\n",
            "|Deep Down (feat. ...|\n",
            "|      1-800-273-8255|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'beautiful summer, wanna make a ride into mountains and find fountains'\n",
        "recommend_system(input, file_df_path, recommend_num).show(recommend_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHfbVcwCGw0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3e963c-99fb-4c96-c383-55b76a65a9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This recommendation spent 3.7 seconds.\n",
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|                 Her|\n",
            "|              Anyone|\n",
            "|                 You|\n",
            "|Hello (feat. A Bo...|\n",
            "|Imagine - Remaste...|\n",
            "|           Our House|\n",
            "|Yeah! (feat. Lil ...|\n",
            "|Players - DJ Smal...|\n",
            "|Deep Down (feat. ...|\n",
            "|            Remember|\n",
            "|            Medicine|\n",
            "|Kill Bill - Sped ...|\n",
            "|                 Top|\n",
            "|Don't You (Forget...|\n",
            "|      1-800-273-8255|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input2 = 'tough days doing internship and homework, need help!'\n",
        "recommend_system(input2, file_df_path, recommend_num).show(recommend_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hold on to your headphones, because things just got a lot faster and more personal! We've cached our system to turn any text input into a customized playlist in **just 5-10 seconds** - that's a stunning leap from our initial 75 seconds. So go on, pick your favourite quote, lyric, or even a line from a book, and let's pump up the volume on your unique, tailor-made playlist!"
      ],
      "metadata": {
        "id": "etCHAw958kHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-sfTaqxjhaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Group_4: Music Recommendation System: Personalized Playlist Generation Using Social Media Content",
      "notebookOrigID": 4339210645485577,
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}