{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2ed803-ed39-4f4f-a600-3ee1b95d4dd4",
   "metadata": {},
   "source": [
    "# Read data from Google Cloud Storage to MonglDB (Atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1670f14-9088-4bbe-a4ff-f2f03c1d2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymongo\n",
    "import pyspark\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dcaa5-96bf-4271-827a-41ac54668f9a",
   "metadata": {},
   "source": [
    "## Connect to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74623efc-fa1f-4845-86b8-6d516172c1a2",
   "metadata": {},
   "source": [
    "#### Local MongoDB configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b9c262-a6ab-4f9d-aa70-a0012240f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ip_address = 'localhost'\n",
    "local_database_name = 'your_database_name'\n",
    "local_collection_name = 'your_collection_name'\n",
    "local_client = pymongo.MongoClient(\"localhost\", 27017)\n",
    "local_db = local_client[local_database_name]\n",
    "local_collection=local_db[local_collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083db788-3c3b-4b5b-a09d-549e3723221f",
   "metadata": {},
   "source": [
    "#### MongoDB Atlas configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee286dc-6288-4aa5-9d0f-ccbda37df508",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_username = 'your_user_name'\n",
    "mongo_password =  'your_password'\n",
    "mongo_ip_address = 'your_id_address.mongodb.net/?retryWrites=true&w=majority'\n",
    "database_name = 'your_database_name'\n",
    "collection_name = 'your_collection_name'\n",
    "client = pymongo.MongoClient(f\"mongodb+srv://{mongo_username}:{mongo_password}@{mongo_ip_address}\")\n",
    "db = client[database_name]\n",
    "collection=db[collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302d714-8772-469d-935a-bc2a74958802",
   "metadata": {},
   "source": [
    "## Connect to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c1f37c-3d76-499a-8d1d-24cf03bac366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 23:00:58 WARN Utils: Your hostname, Xinnnnns-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.107 instead (on interface en0)\n",
      "23/03/22 23:00:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/22 23:00:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 23:01:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/22 23:01:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/03/22 23:01:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'your_bucket_name'\n",
    "service_account_key_file = 'your_service_account_key.json'\n",
    "jar_path='gcs-connector-hadoop2-latest.jar'\n",
    "\n",
    "conf = pyspark.SparkConf().set(\"spark.jars\", jar_path)\n",
    "sc = pyspark.SparkContext(conf=conf).getOrCreate()\n",
    "conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"google.cloud.auth.service.account.json.keyfile\", service_account_key_file)\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ea1b5-7d53-4f4a-9077-80d9fb8c9f47",
   "metadata": {},
   "source": [
    "## Retrieve the song list and tweets from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da3aac-454d-4488-833d-bdd3cd0956a6",
   "metadata": {},
   "source": [
    "If you wish to save data locally, use the 'local_collection' to pull it, but if you want to aggregate data to Atlas, use 'collection' instead. Here I am saving data to Atlas with `collection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3517e-7f09-43fa-98d3-8247b5375b91",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca4863f-d367-423c-ac86-36f71f678fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_new_song_df(spark, bucket_name, mongodb,service_account_key_file):\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "              .option(\"header\", True)\\\n",
    "              .load(\"gs://\"+bucket_name+'/whole.csv')\n",
    "    aggregate_new_songs(mongodb, df)\n",
    "    get_tweet_df(mongodb, spark, bucket_name, service_account_key_file)\n",
    "    \n",
    "\n",
    "def aggregate_new_songs(collection, df):\n",
    "    for row in df.collect():\n",
    "        dict_row=row.asDict()\n",
    "        if list(collection.find({'_id':dict_row['id']}))==[]:\n",
    "            dict_row['_id'] = dict_row['id']\n",
    "            del dict_row['id']\n",
    "            collection.insert_one(dict_row)\n",
    "            \n",
    "\n",
    "def get_tweet_df(collection, spark, bucket_name, service_account_key_file):\n",
    "    storage_client = storage.Client.from_service_account_json(service_account_key_file)\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    song_list = [blob.name for blob in blobs if (blob.name.startswith('2023')) \n",
    "                 and (blob.name.endswith('csv'))]\n",
    "    \n",
    "    for s in song_list:\n",
    "        # Using this line to control the reading process \n",
    "        if s[:10] == '2023-03-21': # and s[11].lower() > 'c':\n",
    "            df = spark.read.format(\"csv\").option(\"header\", True)\\\n",
    "                    .option('multiLine',True)\\\n",
    "                    .load(\"gs://\" + bucket_name + '/' + repr(s).strip('\"').strip(\"'\"))  \n",
    "            song_name = s.replace('.csv','')\n",
    "            print(song_name)\n",
    "            song_name = re.sub(r'^[0-9]+-[0-9]+-[0-9]+'+'/', '', song_name)\n",
    "            for row in df.collect():\n",
    "                dict_row=row.asDict()\n",
    "                push_tweets(\"one\", collection, song_name, dict_row)\n",
    "            print('done')\n",
    "\n",
    "            \n",
    "def push_tweets(type_, collection, song_name, one_content_dict):\n",
    "    if type_ == 'one':\n",
    "        tweet_id=one_content_dict['tweet_id']\n",
    "        if list(collection.find({'tweets.tweet_id':tweet_id}))==[]:\n",
    "            collection.update_one({'name':song_name},\n",
    "                                  {'$push':{'tweets':one_content_dict}})\n",
    "    else:\n",
    "        collection.update_one({'name':song_name},\n",
    "                              {'$push':{'tweets':one_content_dict}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098c192-ad9f-4f9d-9cf2-fe2a49b12f79",
   "metadata": {},
   "source": [
    "#### Read csv file (the song list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f240ab2d-cffd-42ea-ac43-2f1007f5e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"gs://\"+bucket_name+'/whole.csv')\n",
    "aggregate_new_songs(collection, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f283900-c50a-49d4-a3df-af8d3b96bf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79251937-d755-491a-b4b5-3f6164759d57",
   "metadata": {},
   "source": [
    "#### Read folders (tweets for each song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760126f8-64e2-411b-98fb-d4c5879ca5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21/Abadi\n",
      "done\n",
      "2023-03-21/BUBBLY\n",
      "done\n",
      "2023-03-21/Crying On The Dancefloor\n",
      "done\n",
      "2023-03-21/Did you dream too\n",
      "done\n",
      "2023-03-21/Enough\n",
      "done\n",
      "2023-03-21/Gelosa (feat. Shiva, Sfera Ebbasta, Gu√®)\n",
      "done\n",
      "2023-03-21/ICU\n",
      "done\n",
      "2023-03-21/ORIGAMI ALL'ALBA - CLARA\n",
      "done\n",
      "2023-03-21/Players - DJ Smallz 732 - Jersey Club Remix\n",
      "done\n",
      "2023-03-21/Strangest Sea\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21/The April Skies\n",
      "done\n",
      "2023-03-21/To Be Yours (feat. Claud)\n",
      "done\n",
      "2023-03-21/Treat People With Kindness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2023-03-21/Wrapped Around Your Finger\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "get_tweet_df(collection, spark, bucket_name, service_account_key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19b489-3307-4516-8465-28eaba6bcc38",
   "metadata": {},
   "source": [
    "#### Change wired song names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d202c7cc-2cf8-41b3-aab2-9b8995cb599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_collection.update_one({'name':\"NO voy a llorar :')\" },\n",
    "#                             {'$set':{'name':'NO voy a llorar'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c7600-1eb6-4da5-b4c0-7a16e9788283",
   "metadata": {},
   "source": [
    "## Move data from altas to local MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "931c819f-ec36-4e44-a22c-ae17ae062c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_doc = collection.find({})\n",
    "# local_collection.insert_many(list(all_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa29781-9947-405a-9213-e558ec2a43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85754138-2651-4f86-aecc-037c0d2e6db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
